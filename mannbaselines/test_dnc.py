#!/usr/bin/env python
# -*- coding:UTF-8 -*-
# File Name : test.py
# Creation Date : 17-02-2021
# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]

import torch
import torch.nn as nn
from models import NTM, SimpleNTM, RLMEM, MANNMeta

if __name__ == '__main__':
    m = nn.Linear(8, 10)
    ntm = NTM(m, 10, 2, 10).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('NTM lstm done')

    ntm = NTM(m, 10, 2, 10, num_read_heads=3, num_write_heads=3).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('NTM lstm multihead done')

    ntm = NTM(m, 10, 2, 10, controller='mlp', controller_hidden_units=(128, 64)).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('NTM mlp done')

    ntm = NTM(m, 10, 2, 10, controller='mlp', controller_hidden_units=(128, 64), num_read_heads=3, num_write_heads=3).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('NTM mlp multihead done')

    m = nn.Linear(8, 10)
    ntm = SimpleNTM(m, 10, 2, 10).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('SimpleNTM lstm done')

    ntm = SimpleNTM(m, 10, 2, 10, num_read_heads=3, num_write_heads=3).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('SimpleNTM lstm mh done')

    ntm = SimpleNTM(m, 10, 2, 10, controller='mlp', controller_hidden_units=(128, 64)).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('SimpleNTM mlp done')

    ntm = SimpleNTM(m, 10, 2, 10, controller='mlp', controller_hidden_units=(128, 64), num_read_heads=3, num_write_heads=3).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('SimpleNTM mlp mh done')

    m = nn.Linear(8, 10)
    ntm = RLMEM(m, 10, 2, 10).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('RLMEM lstm done')

    ntm = RLMEM(m, 10, 2, 10, num_read_heads=3, num_write_heads=3).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('RLMEM lstm mh done')

    ntm = RLMEM(m, 10, 2, 10, controller='mlp', controller_hidden_units=(128, 64)).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('RLMEM mlp done')

    ntm = RLMEM(m, 10, 2, 10, controller='mlp', controller_hidden_units=(128, 64), num_read_heads=3, num_write_heads=3).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('RLMEM mlp mh done')

    m = nn.Linear(8, 10)
    ntm = MANNMeta(m, 10, 2, 10).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('MANNMeta lstm done')

    ntm = MANNMeta(m, 10, 2, 10, num_read_heads=3, num_write_heads=3).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('MANNMeta lstm mh done')

    ntm = MANNMeta(m, 10, 2, 10, controller='mlp', controller_hidden_units=(128, 64)).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('MANNMeta mlp done')

    ntm = MANNMeta(m, 10, 2, 10, controller='mlp', controller_hidden_units=(128, 64), num_read_heads=3, num_write_heads=3).cuda()
    a = torch.ones(10, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(8, 8).cuda()
    print(a.shape)
    print(ntm.forward_step(a)[0].shape)
    print('===========')
    a = torch.ones(16, 10, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    a = torch.ones(16, 8, 8).cuda()
    print(a.shape)
    print(ntm.forward(a)[0].shape)
    print('===========')
    print('MANNMeta mlp mh done')

